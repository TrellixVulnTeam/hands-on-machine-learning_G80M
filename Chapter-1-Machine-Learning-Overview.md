Some answers to the quetsions asked at the end of Chapter 1. For the sake of authorship, I won't repeat the questions here.

1. *Apprentissage automatique*. Make a computer learn from data without having to explictely program it.
2. Useful for problems with
   1. many different *rules* to get the solution
   2. big data
   3. no obvious definitive answer
   4. moving problem
3. Labeled data, is data for which you have the solution that you want your algorithm to predict
4. Classification and regression are two common supervised tasks.
5. Clustering, anomaly detection, dimension reduction and association rule learning are common unsupervised tasks.
6. Reinforcement learning could be used for a robot to learn exploring.
7. k-means could be used to clusterized clients
8. Spam detection is a supervised task
9. Online learning can learn continuously from new data treated one by one or by mini batches.
10. Out-of-memory learning corresponds to learning where the data can't fir in the memory and where data is split up in batches
11. Observation learning requires a measure of similarity.
12. A model parameter is learned from the data whereas an hyperparameter of an algorithm is defined beofre the learning phase
13. A model based algorithm will defined a best-fit model on training data to then make prediction from new data using this newly paremeterized model
14. Some difficulties that can be encountered: data quantity, data quality, overfitting/bias tradeoff, non representative data
15. Overfitting, non representative data, not enough data
16. Test data set is required for the final evaluation of the chosen model.
17. Validation data set is used to select the best model
18. Train-dev data set is used when the training dataset is known not to be representative of the real-life data and we need to identify whether bad performance are coming from this bad representativity or simply from a poor model
19. Bad performance in real life.
